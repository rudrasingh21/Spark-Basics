{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.117.140:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method text in module pyspark.sql.readwriter:\n",
      "\n",
      "text(paths) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "    string column named \"value\", and followed by partitioned columns if there\n",
      "    are any.\n",
      "    \n",
      "    Each line in the text file is a new row in the resulting DataFrame.\n",
      "    \n",
      "    :param paths: string, or list of strings, for input path(s).\n",
      "    \n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello'), Row(value='this')]\n",
      "    \n",
      "    .. versionadded:: 1.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Reading data from different file formats into data frames:-\n",
    "'''\n",
    "1) JSON\n",
    "2) ORC\n",
    "3) PARQUET\n",
    "4) TEXT\n",
    "'''\n",
    "help(sqlContext.read.json)\n",
    "\n",
    "help(sqlContext.read.orc)\n",
    "\n",
    "help(sqlContext.read.parquet)\n",
    "\n",
    "help(sqlContext.read.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7feefe0d0828>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or Read both are same operation just syntex difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data Frame\n",
    "\n",
    "spark.read.json(\"/home/ubuntu/Data-For-Spark/data-master/retail_db_json/order_items/part-r-00000\").show(2)\n",
    "\n",
    "#sqlContext.read.json(\"/home/ubuntu/Data-For-Spark/data-master/retail_db_json/order_items/part-r-00000\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark.read.json   OR  spark.read.load then pass -->\"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "|            1|                  1|                  957|                  299.98|                  1|             299.98|\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"/home/ubuntu/Data-For-Spark/data-master/retail_db_json/order_items/part-r-00000\",\"json\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load in module pyspark.sql.readwriter:\n",
      "\n",
      "load(path=None, format=None, schema=None, **options) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads data from a data source and returns it as a :class`DataFrame`.\n",
      "    \n",
      "    :param path: optional string or a list of string for file-system backed data sources.\n",
      "    :param format: optional string for format of the data source. Default to 'parquet'.\n",
      "    :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema.\n",
      "    :param options: all other string options\n",
      "    \n",
      "    >>> df = spark.read.load('python/test_support/sql/parquet_partitioned', opt1=True,\n",
      "    ...     opt2=1, opt3='str')\n",
      "    >>> df.dtypes\n",
      "    [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      "    \n",
      "    >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      "    ...     'python/test_support/sql/people1.json'])\n",
      "    >>> df.dtypes\n",
      "    [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkSession in module pyspark.sql.session object:\n",
      "\n",
      "class SparkSession(builtins.object)\n",
      " |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      " |  \n",
      " |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  To create a SparkSession, use the following builder pattern:\n",
      " |  \n",
      " |  >>> spark = SparkSession.builder \\\n",
      " |  ...     .master(\"local\") \\\n",
      " |  ...     .appName(\"Word Count\") \\\n",
      " |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      " |  ...     .getOrCreate()\n",
      " |  \n",
      " |  .. autoattribute:: builder\n",
      " |     :annotation:\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the SparkSession on exit of the with block.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __init__(self, sparkContext, jsparkSession=None)\n",
      " |      Creates a new SparkSession.\n",
      " |      \n",
      " |      >>> from datetime import datetime\n",
      " |      >>> spark = SparkSession(sc)\n",
      " |      >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |      ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |      >>> df = allTypes.toDF()\n",
      " |      >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |      >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |      ...            'from allTypes where b and i > 0').collect()\n",
      " |      [Row((i + CAST(1 AS BIGINT))=2, (d + CAST(1 AS DOUBLE))=2.0, (NOT b)=False, list[1]=2,             dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |      >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |      [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of :class:`Row`,\n",
      " |      or :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      " |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      :param data: an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,\n",
      " |          etc.), or :class:`list`, or :class:`pandas.DataFrame`.\n",
      " |      :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is ``None``.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
      " |          ``int`` as a short name for ``IntegerType``.\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :param verifySchema: verify data types of every row against schema.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 2.1\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> spark.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> spark.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> spark.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = spark.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = spark.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new SparkSession as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared SparkContext and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numPartitions: the number of partitions of the DataFrame\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> spark.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> spark.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Stop the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  catalog\n",
      " |      Interface through which the user may create, drop, alter or query underlying\n",
      " |      databases, tables, functions etc.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  conf\n",
      " |      Runtime configuration interface for Spark.\n",
      " |      \n",
      " |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      " |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      " |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrameReader`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamReader`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  sparkContext\n",
      " |      Returns the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`StreamingQueryManager`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      :return: :class:`UDFRegistration`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      " |      Builder for :class:`SparkSession`.\n",
      " |  \n",
      " |  builder = <pyspark.sql.session.SparkSession.Builder object>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SQLContext in module pyspark.sql.context object:\n",
      "\n",
      "class SQLContext(builtins.object)\n",
      " |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
      " |  \n",
      " |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class\n",
      " |  here for backward compatibility.\n",
      " |  \n",
      " |  A SQLContext can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  \n",
      " |  :param sparkContext: The :class:`SparkContext` backing this SQLContext.\n",
      " |  :param sparkSession: The :class:`SparkSession` around which this SQLContext wraps.\n",
      " |  :param jsqlContext: An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
      " |      SQLContext in the JVM, instead we make all calls to this object.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkContext, sparkSession=None, jsqlContext=None)\n",
      " |      Creates a new SQLContext.\n",
      " |      \n",
      " |      >>> from datetime import datetime\n",
      " |      >>> sqlContext = SQLContext(sc)\n",
      " |      >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |      ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |      >>> df = allTypes.toDF()\n",
      " |      >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |      >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |      ...            'from allTypes where b and i > 0').collect()\n",
      " |      [Row((i + CAST(1 AS BIGINT))=2, (d + CAST(1 AS DOUBLE))=2.0, (NOT b)=False, list[1]=2,             dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |      >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |      [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of :class:`Row`,\n",
      " |      or :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      " |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      :param data: an RDD of any kind of SQL data representation(e.g. :class:`Row`,\n",
      " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      " |          :class:`pandas.DataFrame`.\n",
      " |      :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :param verifySchema: verify data types of every row against schema.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 2.0\n",
      " |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      " |         datatype string after 2.0.\n",
      " |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      " |      \n",
      " |      .. versionchanged:: 2.1\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> sqlContext.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> sqlContext.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = sqlContext.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates an external table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dropTempTable(self, tableName)\n",
      " |      Remove the temp table from catalog.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> sqlContext.dropTempTable(\"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  getConf(self, key, defaultValue=None)\n",
      " |      Returns the value of Spark SQL configuration property for the given key.\n",
      " |      \n",
      " |      If the key is not set and defaultValue is not None, return\n",
      " |      defaultValue. If the key is not set and defaultValue is None, return\n",
      " |      the system default value.\n",
      " |      \n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      " |      '200'\n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n",
      " |      '10'\n",
      " |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", u\"50\")\n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n",
      " |      '50'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared SparkContext and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numPartitions: the number of partitions of the DataFrame\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> sqlContext.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerDataFrameAsTable(self, df, tableName)\n",
      " |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      " |      \n",
      " |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=StringType)\n",
      " |      Registers a python function (including lambda function) as a UDF\n",
      " |      so it can be used in SQL statements.\n",
      " |      \n",
      " |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      " |      When the return type is not given it default to a string and conversion will automatically\n",
      " |      be done.  For any other return type, the produced object must match the specified type.\n",
      " |      \n",
      " |      :param name: name of the UDF\n",
      " |      :param f: python function\n",
      " |      :param returnType: a :class:`pyspark.sql.types.DataType` object\n",
      " |      \n",
      " |      >>> sqlContext.registerFunction(\"stringLengthString\", lambda x: len(x))\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthString('test')\").collect()\n",
      " |      [Row(stringLengthString(test)='4')]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import IntegerType\n",
      " |      >>> sqlContext.registerFunction(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |      [Row(stringLengthInt(test)=4)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import IntegerType\n",
      " |      >>> sqlContext.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
      " |      >>> sqlContext.sql(\"SELECT stringLengthInt('test')\").collect()\n",
      " |      [Row(stringLengthInt(test)=4)]\n",
      " |      \n",
      " |      .. versionadded:: 1.2\n",
      " |  \n",
      " |  registerJavaFunction(self, name, javaClassName, returnType=None)\n",
      " |      Register a java UDF so it can be used in SQL statements.\n",
      " |      \n",
      " |      In addition to a name and the function itself, the return type can be optionally specified.\n",
      " |      When the return type is not specified we would infer it via reflection.\n",
      " |      :param name:  name of the UDF\n",
      " |      :param javaClassName: fully qualified name of java class\n",
      " |      :param returnType: a :class:`pyspark.sql.types.DataType` object\n",
      " |      \n",
      " |      >>> sqlContext.registerJavaFunction(\"javaStringLength\",\n",
      " |      ...   \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
      " |      >>> sqlContext.sql(\"SELECT javaStringLength('test')\").collect()\n",
      " |      [Row(UDF(test)=4)]\n",
      " |      >>> sqlContext.registerJavaFunction(\"javaStringLength2\",\n",
      " |      ...   \"test.org.apache.spark.sql.JavaStringLength\")\n",
      " |      >>> sqlContext.sql(\"SELECT javaStringLength2('test')\").collect()\n",
      " |      [Row(UDF(test)=4)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  setConf(self, key, value)\n",
      " |      Sets the given Spark SQL configuration property.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table or view as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  tableNames(self, dbName=None)\n",
      " |      Returns a list of names of tables in the database ``dbName``.\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use. Default to the current database.\n",
      " |      :return: list of table names, in string\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> \"table1\" in sqlContext.tableNames()\n",
      " |      True\n",
      " |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  tables(self, dbName=None)\n",
      " |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      " |      \n",
      " |      If ``dbName`` is not specified, the current database will be used.\n",
      " |      \n",
      " |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      " |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.tables()\n",
      " |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      " |      Row(database='', tableName='table1', isTemporary=True)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(sc) from builtins.type\n",
      " |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      " |      \n",
      " |      :param sc: SparkContext\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrameReader`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamReader`\n",
      " |      \n",
      " |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      " |      >>> text_sdf.isStreaming\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      :return: :class:`UDFRegistration`\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :func:`spark.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None)\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      :param path: string, or list of strings, for input path(s).\n",
      " |      :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema.\n",
      " |      :param sep: sets the single character as a separator for each field and value.\n",
      " |                  If None is set, it uses the default value, ``,``.\n",
      " |      :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      " |                       it uses the default value, ``UTF-8``.\n",
      " |      :param quote: sets the single character used for escaping quoted values where the\n",
      " |                    separator can be part of the value. If None is set, it uses the default\n",
      " |                    value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      " |                    empty string.\n",
      " |      :param escape: sets the single character used for escaping quotes inside an already\n",
      " |                     quoted value. If None is set, it uses the default value, ``\\``.\n",
      " |      :param comment: sets the single character used for skipping lines beginning with this\n",
      " |                      character. By default (None), it is disabled.\n",
      " |      :param header: uses the first line as names of columns. If None is set, it uses the\n",
      " |                     default value, ``false``.\n",
      " |      :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      " |                     pass over the data. If None is set, it uses the default value, ``false``.\n",
      " |      :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      " |                                      values being read should be skipped. If None is set, it\n",
      " |                                      uses the default value, ``false``.\n",
      " |      :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      " |                                       values being read should be skipped. If None is set, it\n",
      " |                                       uses the default value, ``false``.\n",
      " |      :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      " |                        the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      " |                        applies to all supported types including the string type.\n",
      " |      :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      " |                       uses the default value, ``NaN``.\n",
      " |      :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      " |                          is set, it uses the default value, ``Inf``.\n",
      " |      :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      " |                          is set, it uses the default value, ``Inf``.\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      " |                         set, it uses the default value, ``20480``.\n",
      " |      :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      " |                                value being read. If None is set, it uses the default value,\n",
      " |                                ``-1`` meaning unlimited length.\n",
      " |      :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      " |                                          If specified, it is ignored.\n",
      " |      :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |                   set, it uses the default value, ``PERMISSIVE``.\n",
      " |      \n",
      " |              * ``PERMISSIVE`` : sets other fields to ``null`` when it meets a corrupted                   record, and puts the malformed string into a field configured by                   ``columnNameOfCorruptRecord``. To keep corrupt records, an user can set                   a string type field named ``columnNameOfCorruptRecord`` in an                   user-defined schema. If a schema does not have the field, it drops corrupt                   records during parsing. When a length of parsed CSV tokens is shorter than                   an expected length of a schema, it sets `null` for extra fields.\n",
      " |              * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      " |              * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      " |                                        created by ``PERMISSIVE`` mode. This overrides\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |                                        it uses the value specified in\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      :param multiLine: parse records, which may span multiple lines. If None is\n",
      " |                        set, it uses the default value, ``false``.\n",
      " |      \n",
      " |      >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      " |      >>> df.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      :param source: string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. note:: Don't create too many partitions in parallel on a large cluster;         otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      :param table: the name of the table\n",
      " |      :param column: the name of an integer column that will be used for partitioning;\n",
      " |                     if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
      " |                     (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
      " |                     for generated WHERE clause expressions used to split the column\n",
      " |                     ``column`` evenly\n",
      " |      :param lowerBound: the minimum value of ``column`` used to decide partition stride\n",
      " |      :param upperBound: the maximum value of ``column`` used to decide partition stride\n",
      " |      :param numPartitions: the number of partitions\n",
      " |      :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |                         each one defines one partition of the :class:`DataFrame`\n",
      " |      :param properties: a dictionary of JDBC database connection arguments. Normally at\n",
      " |                         least properties \"user\" and \"password\" with their corresponding values.\n",
      " |                         For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None)\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      :param path: string represents path to the JSON dataset, or a list of paths,\n",
      " |                   or RDD of Strings storing JSON objects.\n",
      " |      :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema.\n",
      " |      :param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
      " |                                 it uses the default value, ``false``.\n",
      " |      :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
      " |                             do not fit in decimal, then it infers them as doubles. If None is\n",
      " |                             set, it uses the default value, ``false``.\n",
      " |      :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
      " |                            it uses the default value, ``false``.\n",
      " |      :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
      " |                                      it uses the default value, ``false``.\n",
      " |      :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
      " |                                      set, it uses the default value, ``true``.\n",
      " |      :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
      " |                                      set, it uses the default value, ``false``.\n",
      " |      :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
      " |                                                 using backslash quoting mechanism. If None is\n",
      " |                                                 set, it uses the default value, ``false``.\n",
      " |      :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |                   set, it uses the default value, ``PERMISSIVE``.\n",
      " |      \n",
      " |              * ``PERMISSIVE`` : sets other fields to ``null`` when it meets a corrupted                  record, and puts the malformed string into a field configured by                  ``columnNameOfCorruptRecord``. To keep corrupt records, an user can set                  a string type field named ``columnNameOfCorruptRecord`` in an user-defined                  schema. If a schema does not have the field, it drops corrupt records during                  parsing. When inferring a schema, it implicitly adds a                  ``columnNameOfCorruptRecord`` field in an output schema.\n",
      " |              *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      " |              *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      " |                                        created by ``PERMISSIVE`` mode. This overrides\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |                                        it uses the value specified in\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
      " |                        set, it uses the default value, ``false``.\n",
      " |      \n",
      " |      >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      " |      >>> df1.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      " |      >>> df2 = spark.read.json(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  load(self, path=None, format=None, schema=None, **options)\n",
      " |      Loads data from a data source and returns it as a :class`DataFrame`.\n",
      " |      \n",
      " |      :param path: optional string or a list of string for file-system backed data sources.\n",
      " |      :param format: optional string for format of the data source. Default to 'parquet'.\n",
      " |      :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema.\n",
      " |      :param options: all other string options\n",
      " |      \n",
      " |      >>> df = spark.read.load('python/test_support/sql/parquet_partitioned', opt1=True,\n",
      " |      ...     opt2=1, opt3='str')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      " |      ...     'python/test_support/sql/people1.json'])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n",
      " |              in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n",
      " |              in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path)\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Currently ORC support is only available together with Hive support.\n",
      " |      \n",
      " |      >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  parquet(self, *paths)\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      You can set the following Parquet-specific option(s) for reading Parquet files:\n",
      " |          * ``mergeSchema``: sets whether we should merge schemas collected from all                 Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``.                 The default value is specified in ``spark.sql.parquet.mergeSchema``.\n",
      " |      \n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  schema(self, schema)\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      :param schema: a :class:`pyspark.sql.types.StructType` object\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :param tableName: string, name of the table.\n",
      " |      \n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.createOrReplaceTempView('tmpTable')\n",
      " |      >>> spark.read.table('tmpTable').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  text(self, paths)\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      \n",
      " |      Each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      :param paths: string, or list of strings, for input path(s).\n",
      " |      \n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello'), Row(value='this')]\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method text in module pyspark.sql.readwriter:\n",
      "\n",
      "text(paths) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      "    string column named \"value\", and followed by partitioned columns if there\n",
      "    are any.\n",
      "    \n",
      "    Each line in the text file is a new row in the resulting DataFrame.\n",
      "    \n",
      "    :param paths: string, or list of strings, for input path(s).\n",
      "    \n",
      "    >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      "    >>> df.collect()\n",
      "    [Row(value='hello'), Row(value='this')]\n",
      "    \n",
      "    .. versionadded:: 1.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n",
      "\n",
      "json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      "    \n",
      "    `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      "    For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      "    \n",
      "    If the ``schema`` parameter is not specified, this function goes\n",
      "    through the input once to determine the input schema.\n",
      "    \n",
      "    :param path: string represents path to the JSON dataset, or a list of paths,\n",
      "                 or RDD of Strings storing JSON objects.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema.\n",
      "    :param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
      "                               it uses the default value, ``false``.\n",
      "    :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
      "                           do not fit in decimal, then it infers them as doubles. If None is\n",
      "                           set, it uses the default value, ``false``.\n",
      "    :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
      "                          it uses the default value, ``false``.\n",
      "    :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
      "                                    it uses the default value, ``false``.\n",
      "    :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
      "                                    set, it uses the default value, ``true``.\n",
      "    :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
      "                                    set, it uses the default value, ``false``.\n",
      "    :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
      "                                               using backslash quoting mechanism. If None is\n",
      "                                               set, it uses the default value, ``false``.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``.\n",
      "    \n",
      "            * ``PERMISSIVE`` : sets other fields to ``null`` when it meets a corrupted                  record, and puts the malformed string into a field configured by                  ``columnNameOfCorruptRecord``. To keep corrupt records, an user can set                  a string type field named ``columnNameOfCorruptRecord`` in an user-defined                  schema. If a schema does not have the field, it drops corrupt records during                  parsing. When inferring a schema, it implicitly adds a                  ``columnNameOfCorruptRecord`` field in an output schema.\n",
      "            *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      "            *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
      "                       applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      "    :param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    \n",
      "    >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      "    >>> df1.dtypes\n",
      "    [('age', 'bigint'), ('name', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      "    >>> df2 = spark.read.json(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('age', 'bigint'), ('name', 'string')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method orc in module pyspark.sql.readwriter:\n",
      "\n",
      "orc(path) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      "    \n",
      "    .. note:: Currently ORC support is only available together with Hive support.\n",
      "    \n",
      "    >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      "    >>> df.dtypes\n",
      "    [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read.orc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(*paths) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      "    \n",
      "    You can set the following Parquet-specific option(s) for reading Parquet files:\n",
      "        * ``mergeSchema``: sets whether we should merge schemas collected from all                 Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``.                 The default value is specified in ``spark.sql.parquet.mergeSchema``.\n",
      "    \n",
      "    >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      "    >>> df.dtypes\n",
      "    [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext.read.parquet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
